Во всех системах уровень логирования по умолчанию - INFO, т.е. все необработанные ошибки попадут в логи с уровнем ERROR или FATAL.

С уровнем INFO логируем как можно больше деталей, но без чувствительных данных:

  * Все операции create/update/delete при взаимодействии с БД
  * Начало/Конец расчета стоимости заказа
  * Изменение статуса обработки заказа
  * События записи/чтения сообщений в брокер сообщений
  * Начало/Конец загрузки модели в файловое хранилище


Мотивация

* Узнавать про проблемы в системе без обращений пользователей
* Восстановить контекст, предшествующий ошибке, что сильно упрощает дебаг проблемы
* Легче и быстрее разбирать инциденты от пользователей, сохраняя их лояльность


Предлагаемое решение

Стандартный стек ELK для сбора и хранения логов, разве что теперь все используют OpenSearch вместо ElasticSearch.

Сбор логов со всех систем будем осуществлять с помощью Filebeat, конвейером данных выступает LogStash, который обрабатывает логи и перенаправляет их на хранение в OpenSearch, визуализация производится с помощью Kibana.

Безопасность

* Доступ к логам осуществляется из внутренней сети или по VPN.
* Доступ к Kibana есть только у техподдержки
* Логи не содержат персональные или чувствительные данные

Политика хранения логов

Желательно ориентироваться на максимальное время выполнения заказа, чтобы в случае инцидента можно было восстановить всю цепочку событий, т.е. от месяца до полугода. При этом всё будет упираться в кол-во логов и стоимость их хранения.

Анализ логов

Необходимо настроить дашборды Kibana так, чтобы можно было визуально искать аномалии - возросшее кол-во ошибок, кол-во OOM, размеры очередей итд. Поверх этих данных настроить алерты с двумя порогами (warning, disaster). Дежурный должен начинать день с ежедневной рутины по изучению логов на предмет ERROR-/FATAL-/WARNING-ошибок и анализа графиков на предмет аномалий.